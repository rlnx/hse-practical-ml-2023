{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Protocol, List, TypeVar, Generic, Optional\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "from my_recsys_metrics import compute_metrics\n",
    "from my_utils import make_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/music_recsys\")\n",
    "train_events = pl.read_parquet(data_path / \"train_events.parquet\")\n",
    "users_for_submission = pl.read_parquet(data_path / \"users_for_submission.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_T = TypeVar(\"_T\")\n",
    "_U = TypeVar(\"_U\")\n",
    "\n",
    "class TransformerLike(Protocol):\n",
    "    def fit_transform(self, input: Any) -> Any: ...\n",
    "\n",
    "\n",
    "class Pipeline(Generic[_T, _U]):\n",
    "    def __init__(self, transformers: List[TransformerLike]) -> None:\n",
    "        self.transformers = transformers\n",
    "\n",
    "    def fit_transform(self, x: _T) -> _U:\n",
    "        y: Any = x\n",
    "        for t in self.transformers:\n",
    "            print(f\"Fit-transform with {t.__class__.__name__}\")\n",
    "            y = t.fit_transform(y)\n",
    "        return y\n",
    "\n",
    "class OrdinalEncoder:\n",
    "    def __init__(self, column: str) -> None:\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df: pl.DataFrame) -> \"OrdinalEncoder\":\n",
    "        self._mapper = (\n",
    "            df[[self.column]].unique()\n",
    "            .sort(self.column)\n",
    "            .with_row_count(\"__index__\")\n",
    "            .with_columns(pl.col(\"__index__\").cast(pl.Int32))\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        df = (\n",
    "            df\n",
    "            .join(self._mapper, on=self.column, how=\"left\")\n",
    "            .drop(self.column)\n",
    "            .rename({\"__index__\": self.column})\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def inverse_transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        df = (\n",
    "            df\n",
    "            .rename({self.column: \"__index__\"})\n",
    "            .join(\n",
    "                self._mapper,\n",
    "                on=\"__index__\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(f\"__index__\")\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "\n",
    "class FilterByPlayRatio:\n",
    "    def __init__(self, min_ratio: float) -> None:\n",
    "        self.min_ratio = min_ratio\n",
    "\n",
    "    def fit_transform(self, events: pl.DataFrame) -> pl.DataFrame:\n",
    "        return events.filter(pl.col(\"play_ratio\") > self.min_ratio)\n",
    "\n",
    "\n",
    "class FrequencyEncoder:\n",
    "    def __init__(self, user_column: str, item_column: str, value_column: str) -> None:\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.value_column = value_column\n",
    "\n",
    "    def fit_transform(self, events: pl.DataFrame) -> pl.DataFrame:\n",
    "        frequency_scores = (\n",
    "            events\n",
    "            .group_by(self.user_column, self.item_column)\n",
    "            .agg(pl.col(self.item_column).count().alias(\"n_interactions_per_user\"))\n",
    "            .with_columns(\n",
    "                pl.col(\"n_interactions_per_user\").sum().over(self.user_column).alias(\"n_interactions_total\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (pl.col(\"n_interactions_per_user\") / pl.col(\"n_interactions_total\")).alias(self.value_column),\n",
    "            )\n",
    "            .drop(\"n_interactions_per_user\", \"n_interactions_total\")\n",
    "        )\n",
    "        return frequency_scores\n",
    "\n",
    "\n",
    "class TFIDFEncoder:\n",
    "    def __init__(self, user_column: str, item_column: str, value_column: str) -> None:\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.value_column = value_column\n",
    "\n",
    "    def fit_transform(self, events: pl.DataFrame) -> pl.DataFrame:\n",
    "        n_users = events[\"user_id\"].n_unique()\n",
    "\n",
    "        def idf_fn(track_occurrences):\n",
    "            return 1 + np.log(1 + 0.001 * (n_users / track_occurrences))\n",
    "\n",
    "        idf_scores = (\n",
    "            events\n",
    "            .group_by(self.item_column)\n",
    "            .agg(pl.col(self.user_column).count().alias(\"n_user_per_item\"))\n",
    "            .with_columns(idf_fn(pl.col(\"n_user_per_item\")).alias(\"idf\"))\n",
    "            .drop(\"n_user_per_item\")\n",
    "        )\n",
    "\n",
    "        tf_scores = (\n",
    "            events\n",
    "            .group_by(self.user_column, self.item_column)\n",
    "            .agg(pl.count().alias(\"n_user_item\"))\n",
    "            .with_columns(\n",
    "                pl.col(\"n_user_item\").sum().over(self.user_column).alias(\"n_total\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (pl.col(\"n_user_item\") / pl.col(\"n_total\")).alias(\"tf\"),\n",
    "            )\n",
    "            .drop(\"n_user_item\", \"n_total\")\n",
    "        )\n",
    "\n",
    "        scores = (\n",
    "            tf_scores\n",
    "            .join(\n",
    "                idf_scores,\n",
    "                on=self.item_column,\n",
    "                how=\"left\"\n",
    "            )\n",
    "            .with_columns((pl.col(\"tf\") * pl.col(\"idf\")).alias(self.value_column))\n",
    "            .drop(\"tf\", \"idf\")\n",
    "        )\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class CSRConverter:\n",
    "    def __init__(self, user_column: str, item_column: str, value_column: str) -> None:\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.value_column = value_column\n",
    "\n",
    "    def fit_transform(self, coo: pl.DataFrame) -> csr_matrix:\n",
    "        user_idx = coo[self.user_column].to_numpy()\n",
    "        item_idx = coo[self.item_column].to_numpy()\n",
    "        values = coo[self.value_column].to_numpy()\n",
    "\n",
    "        n_users = user_idx.max() + 1\n",
    "        n_items = item_idx.max() + 1\n",
    "\n",
    "        user_item_coo = coo_matrix(\n",
    "            (\n",
    "                values.astype(np.float32),\n",
    "                (user_idx, item_idx),\n",
    "            ),\n",
    "            shape=(n_users, n_items),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        user_item_coo.sum_duplicates()\n",
    "\n",
    "        user_item_csr = user_item_coo.tocsr()\n",
    "        return user_item_csr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit-transform with OrdinalEncoder\n",
      "Fit-transform with OrdinalEncoder\n",
      "Fit-transform with FrequencyEncoder\n",
      "Fit-transform with CSRConverter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<12227x115888 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 3277444 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_preprocessing_pipeline: Pipeline[pl.DataFrame, csr_matrix] = Pipeline([\n",
    "    OrdinalEncoder(column=\"user_id\"),\n",
    "    OrdinalEncoder(column=\"track_id\"),\n",
    "    FrequencyEncoder(user_column=\"user_id\", item_column=\"track_id\", value_column=\"freq\"),\n",
    "    CSRConverter(user_column=\"user_id\", item_column=\"track_id\", value_column=\"freq\"),\n",
    "])\n",
    "\n",
    "user_item_csr = events_preprocessing_pipeline.fit_transform(train_events)\n",
    "user_item_csr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:40<00:00,  4.00s/it, loss=0.00029]\n"
     ]
    }
   ],
   "source": [
    "class ALS:\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_column: str,\n",
    "        item_column: str,\n",
    "        score_column: str,\n",
    "        n_factors: int,\n",
    "        n_iterations: int,\n",
    "        top_k: int,\n",
    "    ) -> None:\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.score_column = score_column\n",
    "        self.n_factors = n_factors\n",
    "        self.n_iterations = n_iterations\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        user_item: csr_matrix,\n",
    "        user_item_filter: Optional[csr_matrix] = None,\n",
    "    ) -> pl.DataFrame:\n",
    "        als = AlternatingLeastSquares(\n",
    "            factors=self.n_factors,\n",
    "            iterations=self.n_iterations,\n",
    "            alpha=40.0,\n",
    "            regularization=0.001,\n",
    "            calculate_training_loss=True,\n",
    "        )\n",
    "        als.fit(user_item)\n",
    "\n",
    "        user_ids = np.arange(user_item_csr.shape[0])\n",
    "        recommended_item_indices, recommended_scores = als.recommend(\n",
    "            user_ids,\n",
    "            (user_item_filter if user_item_filter is not None else user_item),\n",
    "            N=self.top_k,\n",
    "            filter_already_liked_items=True,\n",
    "        )\n",
    "\n",
    "        scores_df = pl.DataFrame({\n",
    "            self.user_column: pl.Series(user_ids, dtype=pl.Int32),\n",
    "            self.item_column: pl.Series(recommended_item_indices, dtype=pl.List(pl.Int32)),\n",
    "            self.score_column: pl.Series(recommended_scores, dtype=pl.List(pl.Float32)),\n",
    "        })\n",
    "\n",
    "        scores_df = scores_df.explode(self.item_column, self.score_column)\n",
    "\n",
    "        return scores_df\n",
    "\n",
    "\n",
    "als = ALS(\n",
    "    user_column=\"user_id\",\n",
    "    item_column=\"track_id\",\n",
    "    score_column=\"score\",\n",
    "    n_factors=128,\n",
    "    n_iterations=10,\n",
    "    top_k=10,\n",
    ")\n",
    "als_recommendations = als.fit_predict(user_item_csr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_encoder: OrdinalEncoder = events_preprocessing_pipeline.transformers[0]\n",
    "item_encoder: OrdinalEncoder = events_preprocessing_pipeline.transformers[1]\n",
    "\n",
    "als_recommendations_decoded = als_recommendations\n",
    "als_recommendations_decoded = user_encoder.inverse_transform(als_recommendations_decoded)\n",
    "als_recommendations_decoded = item_encoder.inverse_transform(als_recommendations_decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg@10': 0.025432283035629815, 'recall@10': 0.035414059624585936}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_submission = make_submission(als_recommendations_decoded)\n",
    "compute_metrics(als_submission, pl.read_parquet(data_path / \"ground_truth.parquet\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
