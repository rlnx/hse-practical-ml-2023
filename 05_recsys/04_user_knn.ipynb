{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Protocol, List, TypeVar, Generic\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from scipy.sparse.linalg import norm as sparse_norm\n",
    "from numba import njit, prange\n",
    "\n",
    "from my_recsys_metrics import compute_metrics\n",
    "from my_utils import make_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/music_recsys\")\n",
    "train_events = pl.read_parquet(data_path / \"train_events.parquet\")\n",
    "users_for_submission = pl.read_parquet(data_path / \"users_for_submission.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_T = TypeVar(\"_T\")\n",
    "_U = TypeVar(\"_U\")\n",
    "\n",
    "class TransformerLike(Protocol):\n",
    "    def fit_transform(self, input: Any) -> Any: ...\n",
    "\n",
    "\n",
    "class Pipeline(Generic[_T, _U]):\n",
    "    def __init__(self, transformers: List[TransformerLike]) -> None:\n",
    "        self.transformers = transformers\n",
    "\n",
    "    def fit_transform(self, x: _T) -> _U:\n",
    "        y: Any = x\n",
    "        for t in self.transformers:\n",
    "            print(f\"Fit-transform with {t.__class__.__name__}\")\n",
    "            y = t.fit_transform(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class OrdinalEncoder:\n",
    "    def __init__(self, column: str) -> None:\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, df: pl.DataFrame) -> \"OrdinalEncoder\":\n",
    "        self._mapper = (\n",
    "            df[[self.column]].unique()\n",
    "            .sort(self.column)\n",
    "            .with_row_count(\"__index__\")\n",
    "            .with_columns(pl.col(\"__index__\").cast(pl.Int32))\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        df = (\n",
    "            df\n",
    "            .join(self._mapper, on=self.column, how=\"left\")\n",
    "            .drop(self.column)\n",
    "            .rename({\"__index__\": self.column})\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def inverse_transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        df = (\n",
    "            df\n",
    "            .rename({self.column: \"__index__\"})\n",
    "            .join(\n",
    "                self._mapper,\n",
    "                on=\"__index__\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .drop(f\"__index__\")\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def fit_transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return self.fit(df).transform(df)\n",
    "\n",
    "\n",
    "class FilterByPlayRatio:\n",
    "    def fit_transform(self, events: pl.DataFrame) -> pl.DataFrame:\n",
    "        return events.filter(pl.col(\"play_ratio\") > 0.3)\n",
    "\n",
    "\n",
    "class FrequencyEncoder:\n",
    "    def __init__(self, user_column: str, item_column: str, value_column: str) -> None:\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.value_column = value_column\n",
    "\n",
    "    def fit_transform(self, events: pl.DataFrame) -> pl.DataFrame:\n",
    "        frequency_scores = (\n",
    "            events\n",
    "            .group_by(self.user_column, self.item_column)\n",
    "            .agg(pl.col(self.item_column).count().alias(\"n_interactions_per_user\"))\n",
    "            .with_columns(\n",
    "                pl.col(\"n_interactions_per_user\").sum().over(self.user_column).alias(\"n_interactions_total\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (pl.col(\"n_interactions_per_user\") / pl.col(\"n_interactions_total\")).alias(self.value_column),\n",
    "            )\n",
    "            .drop(\"n_interactions_per_user\", \"n_interactions_total\")\n",
    "        )\n",
    "        return frequency_scores\n",
    "\n",
    "\n",
    "class TFIDFEncoder:\n",
    "    def __init__(self, user_column: str, item_column: str, value_column: str) -> None:\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.value_column = value_column\n",
    "\n",
    "    def fit_transform(self, events: pl.DataFrame) -> pl.DataFrame:\n",
    "        n_users = events[\"user_id\"].n_unique()\n",
    "\n",
    "        idf_scores = (\n",
    "            events\n",
    "            .group_by(self.item_column)\n",
    "            .agg(pl.col(self.user_column).count().alias(\"n_user_per_item\"))\n",
    "            .with_columns(np.log(n_users / pl.col(\"n_user_per_item\")).alias(\"idf\"))\n",
    "            .drop(\"n_user_per_item\")\n",
    "        )\n",
    "\n",
    "        tf_scores = (\n",
    "            events\n",
    "            .group_by(self.user_column, self.item_column)\n",
    "            .agg(pl.count().alias(\"n_user_item\"))\n",
    "            .with_columns(\n",
    "                pl.col(\"n_user_item\").sum().over(self.user_column).alias(\"n_total\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (pl.col(\"n_user_item\") / pl.col(\"n_total\")).alias(\"tf\"),\n",
    "            )\n",
    "            .drop(\"n_user_item\", \"n_total\")\n",
    "        )\n",
    "\n",
    "        scores = (\n",
    "            tf_scores\n",
    "            .join(\n",
    "                idf_scores,\n",
    "                on=self.item_column,\n",
    "                how=\"left\"\n",
    "            )\n",
    "            .with_columns((pl.col(\"tf\") * pl.col(\"idf\")).alias(self.value_column))\n",
    "            .drop(\"tf\", \"idf\")\n",
    "        )\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class CSRConverter:\n",
    "    def __init__(self, user_column: str, item_column: str, value_column: str) -> None:\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.value_column = value_column\n",
    "\n",
    "    def fit_transform(self, coo: pl.DataFrame) -> csr_matrix:\n",
    "        user_idx = coo[self.user_column].to_numpy()\n",
    "        item_idx = coo[self.item_column].to_numpy()\n",
    "        values = coo[self.value_column].to_numpy()\n",
    "\n",
    "        n_users = user_idx.max() + 1\n",
    "        n_items = item_idx.max() + 1\n",
    "\n",
    "        user_item_coo = coo_matrix(\n",
    "            (\n",
    "                values.astype(np.float32),\n",
    "                (user_idx, item_idx),\n",
    "            ),\n",
    "            shape=(n_users, n_items),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        user_item_coo.sum_duplicates()\n",
    "\n",
    "        user_item_csr = user_item_coo.tocsr()\n",
    "        return user_item_csr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit-transform with OrdinalEncoder\n",
      "Fit-transform with OrdinalEncoder\n",
      "Fit-transform with FilterByPlayRatio\n",
      "Fit-transform with TFIDFEncoder\n",
      "Fit-transform with CSRConverter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<12150x115648 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 2137443 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_preprocessing_pipeline: Pipeline[pl.DataFrame, csr_matrix] = Pipeline([\n",
    "    OrdinalEncoder(column=\"user_id\"),\n",
    "    OrdinalEncoder(column=\"track_id\"),\n",
    "    FilterByPlayRatio(),\n",
    "    # FrequencyEncoder(user_column=\"user_id\", item_column=\"track_id\", value_column=\"freq\"),\n",
    "    TFIDFEncoder(user_column=\"user_id\", item_column=\"track_id\", value_column=\"freq\"),\n",
    "    CSRConverter(user_column=\"user_id\", item_column=\"track_id\", value_column=\"freq\"),\n",
    "])\n",
    "\n",
    "user_item_csr = events_preprocessing_pipeline.fit_transform(train_events)\n",
    "user_item_csr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (121_500, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>track_id</th><th>score</th></tr><tr><td>i32</td><td>i32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>107780</td><td>0.009349</td></tr><tr><td>0</td><td>79213</td><td>0.009094</td></tr><tr><td>0</td><td>58821</td><td>0.009133</td></tr><tr><td>0</td><td>25446</td><td>0.009184</td></tr><tr><td>0</td><td>19713</td><td>0.009091</td></tr><tr><td>0</td><td>3050</td><td>0.010239</td></tr><tr><td>0</td><td>105051</td><td>0.009541</td></tr><tr><td>0</td><td>61465</td><td>0.009619</td></tr><tr><td>0</td><td>26279</td><td>0.009081</td></tr><tr><td>0</td><td>15586</td><td>0.009192</td></tr><tr><td>1</td><td>31994</td><td>0.014872</td></tr><tr><td>1</td><td>42072</td><td>0.006508</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>12148</td><td>48222</td><td>0.01872</td></tr><tr><td>12148</td><td>35063</td><td>0.01603</td></tr><tr><td>12149</td><td>45035</td><td>0.007625</td></tr><tr><td>12149</td><td>26074</td><td>0.007432</td></tr><tr><td>12149</td><td>80472</td><td>0.021195</td></tr><tr><td>12149</td><td>70858</td><td>0.006934</td></tr><tr><td>12149</td><td>18297</td><td>0.00897</td></tr><tr><td>12149</td><td>13889</td><td>0.016188</td></tr><tr><td>12149</td><td>14166</td><td>0.010214</td></tr><tr><td>12149</td><td>112853</td><td>0.00728</td></tr><tr><td>12149</td><td>44619</td><td>0.007196</td></tr><tr><td>12149</td><td>25122</td><td>0.007019</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (121_500, 3)\n",
       "┌─────────┬──────────┬──────────┐\n",
       "│ user_id ┆ track_id ┆ score    │\n",
       "│ ---     ┆ ---      ┆ ---      │\n",
       "│ i32     ┆ i32      ┆ f32      │\n",
       "╞═════════╪══════════╪══════════╡\n",
       "│ 0       ┆ 107780   ┆ 0.009349 │\n",
       "│ 0       ┆ 79213    ┆ 0.009094 │\n",
       "│ 0       ┆ 58821    ┆ 0.009133 │\n",
       "│ 0       ┆ 25446    ┆ 0.009184 │\n",
       "│ …       ┆ …        ┆ …        │\n",
       "│ 12149   ┆ 14166    ┆ 0.010214 │\n",
       "│ 12149   ┆ 112853   ┆ 0.00728  │\n",
       "│ 12149   ┆ 44619    ┆ 0.007196 │\n",
       "│ 12149   ┆ 25122    ┆ 0.007019 │\n",
       "└─────────┴──────────┴──────────┘"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Посчитать матрицу похожих пользователей по косинусному расстоянию\n",
    "#    R -- входная матрица [n, m]\n",
    "#    D -- матрица дистанций, d_ij [n, n]\n",
    "#    d_ij = (r_i, r_j) / (|r_i| |r_j|)\n",
    "#    R_norm = norm(R)\n",
    "#    D = (R_norm) x (R_norm)^T\n",
    "#\n",
    "# 2. s_ij = (d_i, r_j) / sum(d_i)\n",
    "#    D_norm = norm(D)\n",
    "#    S = D_norm x R [n, m]\n",
    "\n",
    "class UserBasedKNN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_column: str,\n",
    "        item_column: str,\n",
    "        score_column: str,\n",
    "        n_neighbor_users: int,\n",
    "        top_k: int,\n",
    "    ) -> None:\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.score_column = score_column\n",
    "        self.n_neighbor_users = n_neighbor_users\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def fit_predict(self, user_item: csr_matrix) -> pl.DataFrame:\n",
    "        distances = self._compute_distances(user_item)\n",
    "        scores = self._compute_scores(distances, user_item)\n",
    "        scores_df = self._scores_to_df(scores)\n",
    "        return scores_df\n",
    "\n",
    "    def _scores_to_df(self, scores: csr_matrix) -> pl.DataFrame:\n",
    "        scores_coo = scores.tocoo()\n",
    "        scores_df = pl.DataFrame({\n",
    "            self.user_column: scores_coo.row,\n",
    "            self.item_column: scores_coo.col,\n",
    "            self.score_column: scores_coo.data,\n",
    "        })\n",
    "        return scores_df\n",
    "\n",
    "    def _compute_distances(self, user_item: csr_matrix) -> csr_matrix:\n",
    "        user_item_normalized = self._normalize_user_item(user_item)\n",
    "        distances = user_item_normalized @ user_item_normalized.T\n",
    "        self._keep_largest_per_row_inplace(distances, n_largest=self.n_neighbor_users)\n",
    "        return distances\n",
    "\n",
    "    def _compute_scores(self, distances: csr_matrix, user_item: csr_matrix) -> csr_matrix:\n",
    "        distances_normalized = self._normalize_distances(distances)\n",
    "        scores = distances_normalized @ user_item\n",
    "        self._remove_already_liked_items_inplace(scores, user_item)\n",
    "        self._keep_largest_per_row_inplace(scores, n_largest=self.top_k)\n",
    "        return scores\n",
    "\n",
    "    def _normalize_user_item(self, user_item: csr_matrix) -> csr_matrix:\n",
    "        user_item = user_item.copy()\n",
    "        user_item_norms = sparse_norm(user_item, axis=1)\n",
    "        nnz_per_row = user_item.indptr[1:] - user_item.indptr[:-1]\n",
    "        user_item.data /= np.repeat(user_item_norms, nnz_per_row)\n",
    "        return user_item\n",
    "\n",
    "    def _normalize_distances(self, distances: csr_matrix) -> csr_matrix:\n",
    "        distances = distances.copy()\n",
    "        distances_norms = distances.sum(axis=1).A1\n",
    "        nnz_per_row = distances.indptr[1:] - distances.indptr[:-1]\n",
    "        distances.data /= np.repeat(distances_norms, nnz_per_row)\n",
    "        return distances\n",
    "\n",
    "    def _keep_largest_per_row_inplace(self, distances: csr_matrix, n_largest: int) -> None:\n",
    "       _keep_largest_per_row_nb(\n",
    "           row_count=distances.shape[0],\n",
    "           n_largest=n_largest,\n",
    "           indptr=distances.indptr,\n",
    "           data=distances.data,\n",
    "        )\n",
    "       distances.eliminate_zeros()\n",
    "\n",
    "    def _remove_already_liked_items_inplace(self, scores: csr_matrix, user_item: csr_matrix) -> None:\n",
    "        _remove_already_liked_items_nb(\n",
    "            row_count=scores.shape[0],\n",
    "            scores_data=scores.data,\n",
    "            scores_indptr=scores.indptr,\n",
    "            scores_indices=scores.indices,\n",
    "            interactions_indptr=user_item.indptr,\n",
    "            interactions_indices=user_item.indices,\n",
    "        )\n",
    "        scores.eliminate_zeros()\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def _keep_largest_per_row_nb(\n",
    "    row_count: int,\n",
    "    n_largest: int,\n",
    "    indptr: np.ndarray,\n",
    "    data: np.ndarray,\n",
    ") -> None:\n",
    "    for i in prange(row_count):\n",
    "        row_begin = indptr[i]\n",
    "        row_end = indptr[i + 1]\n",
    "\n",
    "        nnz_per_row = row_end - row_begin\n",
    "        n_to_zero = nnz_per_row - n_largest\n",
    "\n",
    "        if n_to_zero > 0:\n",
    "            indices_to_zero = np.argsort(data[row_begin:row_end])[:n_to_zero]\n",
    "            data[row_begin + indices_to_zero] = 0\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def _remove_already_liked_items_nb(\n",
    "    row_count: int,\n",
    "    scores_data: np.ndarray,\n",
    "    scores_indptr: np.ndarray,\n",
    "    scores_indices: np.ndarray,\n",
    "    interactions_indptr: np.ndarray,\n",
    "    interactions_indices: np.ndarray,\n",
    ") -> None:\n",
    "    for i in prange(row_count):\n",
    "        interactions = interactions_indices[interactions_indptr[i]:interactions_indptr[i + 1]]\n",
    "        for idx in range(scores_indptr[i], scores_indptr[i + 1]):\n",
    "            j = scores_indices[idx]\n",
    "            if j in interactions:\n",
    "                scores_data[idx] = 0.0\n",
    "\n",
    "\n",
    "user_based_knn = UserBasedKNN(\n",
    "    user_column=\"user_id\",\n",
    "    item_column=\"track_id\",\n",
    "    score_column=\"score\",\n",
    "    n_neighbor_users=50,\n",
    "    top_k=10,\n",
    ")\n",
    "userknn_recommendations = user_based_knn.fit_predict(user_item_csr)\n",
    "userknn_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder: OrdinalEncoder = events_preprocessing_pipeline.transformers[0]\n",
    "item_encoder: OrdinalEncoder = events_preprocessing_pipeline.transformers[1]\n",
    "\n",
    "userknn_recommendations_decoded = userknn_recommendations\n",
    "userknn_recommendations_decoded = user_encoder.inverse_transform(userknn_recommendations_decoded)\n",
    "userknn_recommendations_decoded = item_encoder.inverse_transform(userknn_recommendations_decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg@10': 0.017945530826247173, 'recall@10': 0.025911111111111107}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userknn_submission = make_submission(userknn_recommendations_decoded)\n",
    "compute_metrics(userknn_submission, pl.read_parquet(data_path / \"ground_truth.parquet\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
